{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMmpS_Uw8oxD"
      },
      "source": [
        "# Logistic Regression for Credit Card Fraud Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08GlV5_f8oxG"
      },
      "source": [
        "### Loading the data\n",
        "Load the data from `fraud_data.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOCpA6uW8oxH",
        "outputId": "68227f80-3d76-48bc-f035-af0b1d86c0bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of fraud observations: 1.64%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('fraud_data.csv')\n",
        "\n",
        "## Print the percentage of fraud observations\n",
        "\n",
        "X = data.iloc[:,:-1]\n",
        "y = data.iloc[:,-1]\n",
        "\n",
        "# Use X_train, X_test, y_train, y_test for all of the following questions\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "# Calculate and print the percentage of fraud observations in the dataset\n",
        "percentage_fraud = (y.value_counts()[1] / y.count()) * 100\n",
        "print(f\"Percentage of fraud observations: {percentage_fraud:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only 1.64% of observations in the dataset are flagged as fraud. This imbalance indicates that there are far fewer fraud cases compared to legitimate ones. It's a typical scenario in real life where fraud is rare compared to regular transactions."
      ],
      "metadata": {
        "id": "GA_Oo76g_uVw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEOV-bZt8oxI"
      },
      "source": [
        "### Predictions using the majority class label\n",
        "\n",
        "Using `X_train`, `X_test`, `y_train`, and `y_test` (as defined above), train a dummy classifier that classifies everything as the majority class of the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0MwWDbY8oxJ",
        "outputId": "e94321d1-eea9-48a6-f9d1-6ce264f0e5d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy classifier accuray: 0.9852507374631269\n"
          ]
        }
      ],
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "## Instantiate and fit a dummy classifier that always predict class label by the majority class of the training data\n",
        "## Use DummyClassifier in sklearn with strategy 'most_frequent'\n",
        "## Next, use dummpy.fit function to fit the data\n",
        "dummy = DummyClassifier(strategy='most_frequent')\n",
        "\n",
        "# Use dummy.fit function to fit the data\n",
        "dummy.fit(X_train, y_train)\n",
        "\n",
        "dummy_test_pred = dummy.predict(X_test)\n",
        "\n",
        "# Measure test accuracy of the dummy classifier\n",
        "dummy_test_acc = accuracy_score(y_test, dummy_test_pred)\n",
        "\n",
        "print('Dummy classifier accuray:', dummy_test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dummy classifier's accuracy is around 98.52%, which seems impressive. But it's misleading because the dummy classifier just guesses the most common class—in this case, non-fraudulent transactions. Since most transactions aren't fraud, it's right most of the time. But it's not actually detecting fraud; it's just mirroring the dataset's class distribution."
      ],
      "metadata": {
        "id": "dLGu_bLcAOLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dummy classifier's recall score is 0.0, meaning it doesn't correctly classify any fraudulent transactions. This isn't surprising because the dummy classifier always picks the majority class, which here is non-fraudulent transactions. Recall measures a model's ability to find all relevant cases, so a score of 0.0 means the dummy classifier completely misses all fraudulent transactions, which are crucial to detect in this scenario."
      ],
      "metadata": {
        "id": "VFT-Lt8aASx7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1up4M6_8oxK",
        "outputId": "28e63ddf-81d9-473c-ea40-e7d37dbacd52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy classifier recall: 0.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import recall_score\n",
        "\n",
        "# Measure test recall score of your dummy classifier\n",
        "# Since it's a binary classification, we can use the default or set average='binary'\n",
        "dummy_test_recall = recall_score(y_test, dummy_test_pred, average='binary')\n",
        "\n",
        "print('Dummy classifier recall:', dummy_test_recall)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The recall of the dummy classifier is extremely low, zero, because it doesn't catch any of the positive cases, meaning it misses all fraudulent transactions. In fraud detection, recall is crucial—it shows how well the classifier identifies fraud. With a recall of zero, even if the classifier seems accurate due to imbalanced classes, it's ineffective at spotting fraud."
      ],
      "metadata": {
        "id": "Jwdtms4dAVdF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7LjSETR8oxL"
      },
      "source": [
        "### Training a logistic regression model\n",
        "\n",
        "Train a logisitic regression classifier with default parameters using X_train and y_train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkIjM_zt8oxL",
        "outputId": "224d0d8a-00a6-4dec-d117-dd3a1f7b619e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic classifier accuracy: 0.9966814159292036\n",
            "Logistic classifier recall: 0.8\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "\n",
        "# Instantiate a logistic regression model and fit it to the training data\n",
        "logR = LogisticRegression(max_iter=1000) # Increase max_iter if needed for convergence\n",
        "logR.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set using the logistic regression model\n",
        "logR_test_pred = logR.predict(X_test)\n",
        "\n",
        "# Measure test accuracy of the logistic regression model\n",
        "logR_test_acc = accuracy_score(y_test, logR_test_pred)\n",
        "print('Logistic classifier accuracy:', logR_test_acc)\n",
        "\n",
        "# Measure test recall of the logistic regression model\n",
        "logR_test_recall = recall_score(y_test, logR_test_pred, average='binary')\n",
        "print('Logistic classifier recall:', logR_test_recall)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The logistic regression model performs far better than the dummy classifier in terms of recall. It achieves a recall of 0.8, meaning it accurately detects 80% of fraudulent transactions. In contrast, the dummy classifier has a recall of 0.0, meaning it doesn't catch any fraudulent transactions.\n",
        "\n",
        "Although both classifiers boast high accuracy scores (approximately 98.52% for the dummy and 99.67% for logistic regression), recall is the crucial metric for fraud detection. It's more important to catch fraud, even if it means wrongly flagging some legitimate transactions. The logistic regression model's high recall suggests it's much more effective for fraud detection, despite the dataset's imbalance. On the other hand, the dummy classifier's high accuracy is misleading—it mainly reflects the prevalence of the majority class, not its fraud detection ability."
      ],
      "metadata": {
        "id": "DlIeKMPFBSEV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDxJg6uB8oxM"
      },
      "source": [
        "### Grid search for selecting hyperparameters for Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLBrESXR8oxN",
        "outputId": "248b4e0a-16a6-4aa5-b166-f987435eba33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic classifier accuracy: 0.9963126843657817\n",
            "Logistic classifier recall: 0.775\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "## Define the grid of logistic regression parameters\n",
        "# Define the grid of logistic regression parameters\n",
        "parameters = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],  # Norm used in the penalization\n",
        "    'solver': ['liblinear']  # Solver that supports both l1 and l2 penalties\n",
        "}\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "## Perform grid search CV to find best model parameter setting\n",
        "cmodel = GridSearchCV(model, parameters, cv=5)  # cv=5 for 5-fold cross-validation\n",
        "cmodel.fit(X_train, y_train.ravel())\n",
        "\n",
        "## Fit logistic regression with best parameters to the entire training data\n",
        "best_params = cmodel.best_params_\n",
        "model = LogisticRegression(**best_params, max_iter=1000)\n",
        "model.fit(X_train, y_train.ravel())\n",
        "\n",
        "logR_test_pred = model.predict(X_test)\n",
        "\n",
        "# Measure test accuracy\n",
        "logR_test_acc = accuracy_score(y_test, logR_test_pred)\n",
        "print('Logistic classifier accuracy:', logR_test_acc)\n",
        "\n",
        "# Measure test recall\n",
        "logR_test_recall = recall_score(y_test, logR_test_pred, average='binary')\n",
        "print('Logistic classifier recall:', logR_test_recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The logistic regression model with default parameters performed slightly better than the model optimized via grid search. Default parameters achieved about 99.668% accuracy and a recall of 0.8, while the grid search model had about 99.631% accuracy and a recall of 0.775.\n",
        "\n",
        "The higher recall with default parameters suggests it was better at catching fraud (true positives). Though differences are small, they indicate the default model was slightly more effective at classifying fraudulent transactions. However, depending on the cost of false negatives versus false positives, the trade-offs between recall and precision may guide final model selection.\n",
        "\n",
        "This comparison highlights the importance of considering multiple metrics, especially in imbalanced classification tasks. While grid search helps with hyperparameter tuning, it doesn't always guarantee better performance than default settings, particularly if optimization metrics don't align perfectly with practical objectives."
      ],
      "metadata": {
        "id": "o-uWthw5A-KB"
      }
    }
  ],
  "metadata": {
    "coursera": {
      "course_slug": "python-machine-learning",
      "graded_item_id": "5yX9Z",
      "launcher_item_id": "eqnV3",
      "part_id": "Msnj0"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}